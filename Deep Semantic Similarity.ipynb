{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796fcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_trigrams():\n",
    "  \"\"\"\n",
    "      Generates all trigrams for characters from `trigram_chars`\n",
    "  \"\"\"\n",
    "  trigram_chars=\"0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "  t3=[''.join(x) for x in itertools.product(trigram_chars,repeat=3)] #len(words)>=3\n",
    "  t2_start=['#'+''.join(x) for x in itertools.product(trigram_chars,repeat=2)] #len(words)==2\n",
    "  t2_end=[''.join(x)+'#' for x in itertools.product(trigram_chars,repeat=2)] #len(words)==2\n",
    "  t1=['#'+''.join(x)+'#' for x in itertools.product(trigram_chars)] #len(words)==1\n",
    "  trigrams=t3+t2_start+t2_end+t1\n",
    "  vocab_size=len(trigrams)\n",
    "  trigram_map=dict(zip(trigrams,range(1,vocab_size+1))) # trigram to index mapping, indices starting from 1\n",
    "  return trigram_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29ebbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_bag_of_trigrams(sentences):\n",
    "  \"\"\"\n",
    "      Converts a sentence to bag-of-trigrams\n",
    "      `sentences`: list of strings\n",
    "      `trigram_BOW`: return value, (len(sentences),len(trigram_map)) size array\n",
    "  \"\"\"\n",
    "  trigram_map=gen_trigrams()\n",
    "  trigram_BOW=np.zeros((len(sentences),len(trigram_map))) # one row for each sentence\n",
    "  filter_pat=r'[\\!\"#&\\(\\)\\*\\+,-\\./:;<=>\\?\\[\\\\\\]\\^_`\\{\\|\\}~\\t\\n]' # characters to filter out from the input\n",
    "  for j,sent in enumerate(sentences):\n",
    "      sent=re.sub(fiter_pat, '', sent).lower() # filter out special characters from input\n",
    "      sent=re.sub(r\"(\\s)\\s+\", r\"\\1\", sent) # reduce multiple whitespaces to single whitespace\n",
    "      words=sent.split(' ')\n",
    "      indices=collections.defaultdict(int)\n",
    "      for word in words:\n",
    "          word='#'+word+'#'\n",
    "          #print(word)\n",
    "          for k in range(len(word)-2): # generate all trigrams for word `word` and update `indices`\n",
    "              trig=word[k:k+3]\n",
    "              idx=trigram_map.get(trig, 0)\n",
    "              #print(trig,idx)\n",
    "              indices[idx]=indices[idx]+1     \n",
    "      for key,val in indices.items(): #covert `indices` dict to np array\n",
    "          trigram_BOW[j,key]=val\n",
    "  return trigram_BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6141ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_layer(X,INP_NEURONS,OUT_NEURONS, X_is_sparse=False):\n",
    "     \"\"\"\n",
    "         Create a Fully Connected layer\n",
    "         `X`: input array/activations of previous layer\n",
    "         `INP_NEURONS`: number of neurons in previous layer\n",
    "         `OUT_NEURONS`: number of neurons in this layer\n",
    "         `X_is_sparse`: bool value to indicate if input `X` is sparse or not.\n",
    "                        Default value is False\n",
    "\n",
    "     \"\"\"\n",
    "     limit=np.sqrt(6.0/(INP_NEURONS+OUT_NEURONS))\n",
    "     W=tf.Variable(tf.random_uniform(\n",
    "         (INP_NEURONS,OUT_NEURONS), -limit, limit), #weight init\n",
    "         name=\"weight\")\n",
    "     b=tf.Variable(tf.random_uniform((OUT_NEURONS),-limit, limit), name=\"bias\") # bias\n",
    "     prod=tf.sparse_tensor_dense_matmul(X,W) if X_is_sparse else tf.matmul(X,W) #linear transformation\n",
    "     return tf.nn.tanh(prod+b) # non-linear (tanh) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a9060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(self,A,B):\n",
    "     \"\"\"\n",
    "         Function to calculate cosine similarity between two (batches of) vectors\n",
    "         `A` and  `B`: Inputs (shape => [batch_size,vector_size])\n",
    "         `sim`: Return value, cosine similarity of A and B\n",
    "     \"\"\"\n",
    "     Anorm=tf.nn.l2_normalize(A, dim=1) # normalize A\n",
    "     Bnorm=tf.nn.l2_normalize(B, dim=1) # normalize B\n",
    "     sim=tf.reduce_sum(Anorm*Bnorm, axis=1) # dot product of normalized A and normalized B\n",
    "     return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11be2f73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-887c85d66041>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_cosine_similarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "smax=tf.nn.softmax_cross_entropy_with_logits(logits=batch_cosine_similarities, labels=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ee2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
